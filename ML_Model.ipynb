{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eBay O*14-xxxxx-03977 - Visa Purchase - Receip...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eBay O*14-xxxxx-03978 - Visa Purchase - Receip...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eBay O*14-xxxxx-03976 - Visa Purchase - Receip...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eBay O*14-xxxxx-03975 - Visa Purchase - Receip...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ROLLD KOTARA - Visa Purchase - Receipt ${recei...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description            tag\n",
       "0  eBay O*14-xxxxx-03977 - Visa Purchase - Receip...  miscellaneous\n",
       "1  eBay O*14-xxxxx-03978 - Visa Purchase - Receip...  miscellaneous\n",
       "2  eBay O*14-xxxxx-03976 - Visa Purchase - Receip...  miscellaneous\n",
       "3  eBay O*14-xxxxx-03975 - Visa Purchase - Receip...  miscellaneous\n",
       "4  ROLLD KOTARA - Visa Purchase - Receipt ${recei...           food"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "import pandas as pd\n",
    "ds_import = pd.read_json('ready-dataset.json')\n",
    "ds_import.to_json(orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "# Shape data\n",
    "ds_import.shape\n",
    "df = ds_import[['description', 'tag']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entertainment: 43\n",
      "food: 420\n",
      "rent: 23\n",
      "miscellaneous: 271\n",
      "electronics: 26\n",
      "industrial: 48\n",
      "residency: 29\n",
      "health: 57\n",
      "max value: 131.15258289488622\n"
     ]
    }
   ],
   "source": [
    "# https://www.w3schools.com/python/python_ml_standard_deviation.asp\n",
    "import math\n",
    "\n",
    "counts = df['tag'].value_counts().to_dict()\n",
    "\n",
    "if('electricity' in counts):\n",
    "    print('electricity: ' + counts['electricity'])\n",
    "if('gas' in counts):\n",
    "    print('gas: ' + str(counts['gas']))\n",
    "\n",
    "print('entertainment: ' + str(counts['entertainment']))\n",
    "print('food: ' + str(counts['food']))\n",
    "print('rent: ' + str(counts['rent']))\n",
    "print('miscellaneous: ' + str(counts['miscellaneous']))\n",
    "print('electronics: ' + str(counts['electronics']))\n",
    "print('industrial: ' + str(counts['industrial']))\n",
    "print('residency: ' + str(counts['residency']))\n",
    "print('health: ' + str(counts['health']))\n",
    "\n",
    "# calculating mean\n",
    "meanTagCount = int(sum(counts.values())/len(counts))\n",
    "\n",
    "# difference from mean for each value\n",
    "counts['entertainment'] = math.pow(counts['entertainment'] - meanTagCount, 2)\n",
    "counts['food'] = math.pow(counts['food'] - meanTagCount, 2)\n",
    "counts['rent'] = math.pow(counts['rent'] - meanTagCount, 2)\n",
    "counts['miscellaneous'] = math.pow(counts['miscellaneous'] - meanTagCount, 2)\n",
    "counts['electronics'] = math.pow(counts['electronics'] - meanTagCount, 2)\n",
    "counts['industrial'] = math.pow(counts['industrial'] - meanTagCount, 2)\n",
    "counts['residency'] = math.pow(counts['residency'] - meanTagCount, 2)\n",
    "counts['health'] = math.pow(counts['health'] - meanTagCount, 2)\n",
    "\n",
    "# calculating standard deviation\n",
    "standardDeviation = math.pow(int(sum(counts.values())/len(counts)), 0.5)\n",
    "print(\"max value: \" + str(standardDeviation))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food size: 420\n",
      "standard deviation: 131.15258289488622\n",
      "food size: 131\n"
     ]
    }
   ],
   "source": [
    "# delete random rows from columns until max (standardDeviation) has been reaced\n",
    "import random\n",
    "\n",
    "print(\"food size: \" + str(df[((df.tag == \"food\"))].index.size))\n",
    "print(\"standard deviation: \" + str(standardDeviation))\n",
    "\n",
    "# food\n",
    "while df[((df.tag == \"food\"))].index.size > standardDeviation:\n",
    "    i = df[((df.tag == \"food\"))].index\n",
    "    df = df.drop(random.choice(i))\n",
    "print(\"food size: \" + str(df[((df.tag == \"food\"))].index.size))\n",
    "\n",
    "# miscellaneous\n",
    "while df[((df.tag == \"miscellaneous\"))].index.size > standardDeviation:\n",
    "    i = df[((df.tag == \"miscellaneous\"))].index\n",
    "    df = df.drop(random.choice(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entertainment: 43\n",
      "food: 131\n",
      "rent: 23\n",
      "miscellaneous: 131\n",
      "electronics: 26\n",
      "industrial: 48\n",
      "residency: 29\n",
      "health: 57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counts = df['tag'].value_counts().to_dict()\n",
    "\n",
    "if('electricity' in counts):\n",
    "    print('electricity: ' + counts['electricity'])\n",
    "if('gas' in counts):\n",
    "    print('gas: ' + str(counts['gas']))\n",
    "\n",
    "print('entertainment: ' + str(counts['entertainment']))\n",
    "print('food: ' + str(counts['food']))\n",
    "print('rent: ' + str(counts['rent']))\n",
    "print('miscellaneous: ' + str(counts['miscellaneous']))\n",
    "print('electronics: ' + str(counts['electronics']))\n",
    "print('industrial: ' + str(counts['industrial']))\n",
    "print('residency: ' + str(counts['residency']))\n",
    "print('health: ' + str(counts['health']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(row):\n",
    "    text = row['description']\n",
    "    text = str(text)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    encodings = tokenizer(text, padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "    label = 0\n",
    "    if row['tag'] == 'electricity':\n",
    "        label = 0\n",
    "    if row['tag'] == 'gas':\n",
    "        label = 1\n",
    "    if row['tag'] == 'entertainment':\n",
    "        label = 2\n",
    "    if row['tag'] == 'food':\n",
    "        label = 3\n",
    "    if row['tag'] == 'rent':\n",
    "        label = 4\n",
    "    if row['tag'] == 'miscellaneous':\n",
    "        label = 5\n",
    "    if row['tag'] == 'electronics':\n",
    "        label = 6\n",
    "    if row['tag'] == 'industrial':\n",
    "        label = 7\n",
    "    if row['tag'] == 'residency':\n",
    "        label = 8\n",
    "    if row['tag'] == 'health':\n",
    "        label = 9\n",
    "    \n",
    "    encodings['label'] = label\n",
    "    encodings['text'] = text\n",
    "\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1041, 15907, 12598, 8909, 2173, 2193, 2518, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 9, 'text': 'ebay transaction id place number thing'}\n"
     ]
    }
   ],
   "source": [
    "print(process_data({\n",
    "    'description': 'ebay transaction id place number thing',\n",
    "    'tag': 'health'\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "\n",
    "for i in range(len(df[:100])):\n",
    "    processed_data.append(process_data(df.iloc[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "new_df = pd.DataFrame(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_df, valid_df = train_test_split(\n",
    "    new_df,\n",
    "    test_size=0.2,\n",
    "    random_state=2022\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from datasets import Dataset\n",
    "\n",
    "train_hg = Dataset(pa.Table.from_pandas(train_df))\n",
    "valid_hg = Dataset(pa.Table.from_pandas(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./result\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_hg,\n",
    "        eval_dataset=valid_hg,\n",
    "        tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\moosa\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 80\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5be147af52542dabe28ee87b82fa967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d037ebbe0b5c464db4989a00f4a799fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.322967529296875, 'eval_runtime': 3.568, 'eval_samples_per_second': 5.605, 'eval_steps_per_second': 0.841, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b812e2a4be489b94923e56f0328be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1549410820007324, 'eval_runtime': 3.55, 'eval_samples_per_second': 5.634, 'eval_steps_per_second': 0.845, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63631568d1984d8faa6b3221e4c88b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.067741632461548, 'eval_runtime': 3.594, 'eval_samples_per_second': 5.565, 'eval_steps_per_second': 0.835, 'epoch': 3.0}\n",
      "{'train_runtime': 171.8965, 'train_samples_per_second': 1.396, 'train_steps_per_second': 0.175, 'train_loss': 2.1237764994303387, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=2.1237764994303387, metrics={'train_runtime': 171.8965, 'train_samples_per_second': 1.396, 'train_steps_per_second': 0.175, 'train_loss': 2.1237764994303387, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8f2568936b42b6a45970cdb9c72f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.067741632461548,\n",
       " 'eval_runtime': 3.484,\n",
       " 'eval_samples_per_second': 5.741,\n",
       " 'eval_steps_per_second': 0.861,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./model/config.json\n",
      "Model weights saved in ./model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('./model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./model/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "new_model = AutoModelForSequenceClassification.from_pretrained('./model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\moosa/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\moosa/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\moosa/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\moosa/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\moosa/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "new_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_prediction(text):\n",
    "    encoding = new_tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "    encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "    outputs = new_model(**encoding)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    probs = probs.detach().numpy()\n",
    "    label = np.argmax(probs, axis=-1)\n",
    "\n",
    "    if label == 0:\n",
    "        return {\n",
    "            'tag': 'electricity',\n",
    "            'probability': probs[0]\n",
    "        }\n",
    "    if label == 1:\n",
    "        return {\n",
    "            'tag': 'gas',\n",
    "            'probability': probs[1]\n",
    "        }\n",
    "    if label == 2:\n",
    "        return {\n",
    "            'tag': 'entertainment',\n",
    "            'probability': probs[1]\n",
    "        }\n",
    "    if label == 3:\n",
    "        return {\n",
    "            'tag': 'food',\n",
    "            'probability': probs[1]\n",
    "        }\n",
    "    if label == 4:\n",
    "        return {\n",
    "            'tag': 'rent',\n",
    "            'probability': probs[1]\n",
    "        }\n",
    "    if label == 5:\n",
    "        return {\n",
    "            'tag': 'miscellaneous',\n",
    "            'probability': probs[1]\n",
    "        }\n",
    "    if label == 6:\n",
    "        return {\n",
    "            'tag': 'electronics',\n",
    "            'probability': probs[1]\n",
    "        }\n",
    "    if label == 7:\n",
    "        return {\n",
    "            'tag': 'industrial',\n",
    "            'probability': probs[1]\n",
    "        }\n",
    "    if label == 8:\n",
    "        return {\n",
    "            'tag': 'residency',\n",
    "            'probability': probs[1]\n",
    "        }\n",
    "    if label == 9:\n",
    "        return {\n",
    "            'tag': 'health',\n",
    "            'probability': probs[1]\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tag': 'miscellaneous', 'probability': 0.41808972}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Miscellanous prediction\n",
    "get_prediction('AMAZON MKTPLC AU - Visa Purchase - Receipt ${receipt_no} SYDNEY SOUTH ${trans_date} 2022 Card 491684xxxxxx8665')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tag': 'food', 'probability': 0.35618553}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Food prediction\n",
    "get_prediction('COLES 0746 - Visa Purchase - Receipt ${receipt_no} WARATAH Date ${trans_date} Card 491684xxxxxx8665')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.982492276004123\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy of all predictions and actual values\n",
    "import pandas as pd\n",
    "\n",
    "ds_import = pd.read_json('ready-dataset.json')\n",
    "ds_import.to_json(orient=\"records\", lines=True)\n",
    "ds_import.shape\n",
    "df = ds_import[['description', 'tag']]\n",
    "\n",
    "length = len(df.index)\n",
    "\n",
    "correct = 0\n",
    "for index, row in df.iterrows():\n",
    "    predictTag = get_prediction(row['description'])\n",
    "    if(str(predictTag['tag']) == row['tag']):\n",
    "        correct = correct + 1\n",
    "\n",
    "accuracy = (correct / length) * 100\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f11852ad9eb5dcf9a8fb6ac4002145841e272b4e87cc4ecedba8dfbe7d343d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
